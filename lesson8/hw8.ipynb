{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9VSVFRQPT/ay/+ZgRhPAM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddekun/Intro_Neural_Networks/blob/lesson8/lesson8/hw8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Введение в нейронные сети"
      ],
      "metadata": {
        "id": "0PSzBgPsyklg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Урок 8. GAN"
      ],
      "metadata": {
        "id": "8bV9o-VHzUpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Обучите нейронную сеть любой архитектуры, которой не было на курсе, либо нейронную сеть разобранной архитектуры, но на том датасете, которого не было на уроках. Сделайте анализ того, что вам помогло в улучшения работы нейронной сети"
      ],
      "metadata": {
        "id": "NqRqZ-EUzWP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "jv2BaAY9BOpY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Строим слой семплирования sampling layer**"
      ],
      "metadata": {
        "id": "-1Ng0ZZ3BSY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# стороим кастомный слой\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a unit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs  # с двумя параметрами на входе\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1] # определяем размер нашего пространства\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim)) # строим шум\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon # восстанавливаем пакет единиц информации, опираясь на параметры нашего распр"
      ],
      "metadata": {
        "id": "Yd_yIbhDBRoq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Строим энкодер encoder**"
      ],
      "metadata": {
        "id": "o8P_v8LABVvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 2 # берем пространство равное 2 для быстроты\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
        "# строим сверточную модель\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "# строим выходы\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "# получилась модель: на входе картинка, на выходе 3 тензора\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CoxEH6LBUdq",
        "outputId": "190ca04f-15e3-4f6f-a96e-90aa2f38058b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 14, 14, 32)   320         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 7, 7, 64)     18496       ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 3136)         0           ['conv2d_11[0][0]']              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 16)           50192       ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 2)            34          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 2)            34          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " sampling (Sampling)            (None, 2)            0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 69,076\n",
            "Trainable params: 69,076\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Строим свой decoder**"
      ],
      "metadata": {
        "id": "3BU7RofpBaS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "# размерность 7 * 7 * 64 позволяет потом построить выход 28\n",
        "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "# превращаем в тензор более сложной формы 4-мерный\n",
        "x = layers.Reshape((7, 7, 64))(x)\n",
        "# как тольо сказали strides=2, то увеличили ращмерность в 2 раза\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x) # 14x14\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x) # 28x28\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x) # одноканальная картинка 28x28\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwPHLWu_BbsK",
        "outputId": "b135345c-4179-425a-e9f3-734be1ba43da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 2)]               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3136)              9408      \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 7, 7, 64)          0         \n",
            "                                                                 \n",
            " conv2d_transpose_8 (Conv2DT  (None, 14, 14, 64)       36928     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_9 (Conv2DT  (None, 28, 28, 32)       18464     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_10 (Conv2D  (None, 28, 28, 1)        289       \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 65,089\n",
            "Trainable params: 65,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Создаем класс модель по Model с особым шагом обучения train_step**"
      ],
      "metadata": {
        "id": "qckDMx_xBesh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# создаем модель как наследник класса Model\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def train_step(self, data): # меняем на свое обучение функцию train_step\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "        with tf.GradientTape() as tape: # создаем объект для дифферецнирования\n",
        "            z_mean, z_log_var, z = encoder(data)  # картинки пропускаем через энкодер\n",
        "            reconstruction = decoder(z) # пропускаем через декодер\n",
        "            # строим первый loss\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                keras.losses.binary_crossentropy(data, reconstruction)\n",
        "            )\n",
        "            reconstruction_loss *= 28 * 28\n",
        "            # строим второй loss\n",
        "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "            kl_loss = tf.reduce_mean(kl_loss)\n",
        "            kl_loss *= -0.5 # коэффициент обеспечивает равный вклад двух лоссов\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights) # говорим, что у модели будут тренироваться веса\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        return {\n",
        "            \"loss\": total_loss,\n",
        "            \"reconstruction_loss\": reconstruction_loss,\n",
        "            \"kl_loss\": kl_loss,\n",
        "        }"
      ],
      "metadata": {
        "id": "WvJ6wm9qBfth"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Учим VAE**"
      ],
      "metadata": {
        "id": "FDzuhVHJBjgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, _), (x_test, _) = keras.datasets.fashion_mnist.load_data() # подгружаем FASHION-MNIST\n",
        "mnist_units = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_units = np.expand_dims(mnist_units, -1).astype(\"float32\") / 255\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam()) # задаем оптимайзер\n",
        "vae.fit(mnist_units, epochs=30, batch_size=128) # проводим обучение"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJaPuVYlBh9i",
        "outputId": "8a8092ff-fb4f-47de-e4a1-5f85db7cab6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n",
            "Epoch 1/30\n",
            "547/547 [==============================] - 150s 271ms/step - loss: 317.6309 - reconstruction_loss: 312.1427 - kl_loss: 5.4882\n",
            "Epoch 2/30\n",
            "547/547 [==============================] - 141s 258ms/step - loss: 277.2750 - reconstruction_loss: 272.9078 - kl_loss: 4.3672\n",
            "Epoch 3/30\n",
            "547/547 [==============================] - 148s 271ms/step - loss: 272.2064 - reconstruction_loss: 268.2615 - kl_loss: 3.9449\n",
            "Epoch 4/30\n",
            "163/547 [=======>......................] - ETA: 1:35 - loss: 270.3435 - reconstruction_loss: 266.4824 - kl_loss: 3.8610"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Результат предсказания для датасета одежды**"
      ],
      "metadata": {
        "id": "_m3Pj6ZKB0hR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# на этапе предсказания энкодер не нужен\n",
        "# По одной точке из скрытого пространства восстанавливаем объект, подавая через декодер\n",
        "def plot_latent(encoder, decoder):\n",
        "    # display a n*n 2D manifold of units\n",
        "    n = 30\n",
        "    unit_size = 28\n",
        "    scale = 2.0 # диапазон пространства от минус 2 до 2\n",
        "    figsize = 15\n",
        "    figure = np.zeros((unit_size * n, unit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of unit classes in the latent space\n",
        "    grid_x = np.linspace(-scale, scale, n)\n",
        "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = decoder.predict(z_sample)\n",
        "            unit = x_decoded[0].reshape(unit_size, unit_size)\n",
        "            figure[\n",
        "                i * unit_size : (i + 1) * unit_size,\n",
        "                j * unit_size : (j + 1) * unit_size,\n",
        "            ] = unit\n",
        "\n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    start_range = unit_size // 2\n",
        "    end_range = n * unit_size + start_range + 1\n",
        "    pixel_range = np.arange(start_range, end_range, unit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap=\"Greys_r\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_latent(encoder, decoder)"
      ],
      "metadata": {
        "id": "uE4tIr00Bzvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Посмотрим на кластеры в скрытом пространстве для одежды**"
      ],
      "metadata": {
        "id": "cXFCFJPgB6I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_label_clusters(encoder, data, labels):\n",
        "    # display a 2D plot of the unit classes in the latent space\n",
        "    z_mean, _, _ = encoder.predict(data)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "(x_train, y_train), _ = keras.datasets.fashion_mnist.load_data()\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
        "\n",
        "plot_label_clusters(encoder, x_train, y_train)"
      ],
      "metadata": {
        "id": "BEXT16z8B7Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Построим немного примеров генерации одежды:\n",
        "\n",
        "для этого не нужен энкодер работа этой модели требует:\n",
        "- выбрать случайные значения для точки скрытого пространства (2 координаты (среднее и логарифм от дисперсиии) - для построения в n (в нашем случае 28х28) координаты нового образца )\n",
        "- подать их в декодер\n",
        "- визуализировать результат"
      ],
      "metadata": {
        "id": "6RM_FuSYB_WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_unit_from_decoder(decoder, z, unit_size):\n",
        "    # display a 2D plot of the unit classes in the latent space\n",
        "\n",
        "    data = np.array([[z[0], z[1]]])\n",
        "    unit = decoder.predict(data)\n",
        "    unit = unit.reshape(unit_size, unit_size)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(unit)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_unit_from_decoder(decoder, [1.4,-1.3], 28)"
      ],
      "metadata": {
        "id": "WVlB3i5LCJjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Увеличим размер скрытого пространства**"
      ],
      "metadata": {
        "id": "mXXvdZZJCNmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 4\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(128, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "VpN4ddNvCPmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7 * 7 * 128, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((7, 7, 128))(x)\n",
        "x = layers.Conv2DTranspose(128, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()"
      ],
      "metadata": {
        "id": "TgCo1y6CCRi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam()) # задаем оптимайзер\n",
        "vae.fit(mnist_units, epochs=30, batch_size=128) # проводим обучение"
      ],
      "metadata": {
        "id": "AA8ThL9PCURI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "По сравнению с сетью с 2 скрытыми слоями - сети с 6 скрытыми слоями удалось добиться уменьшения обеих составляющих ошибки. Лоссы стабилизировались."
      ],
      "metadata": {
        "id": "bKu-xfh2CXkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Визуализируем полученные результаты, зафиксировав несколько измерений.**"
      ],
      "metadata": {
        "id": "iN4o-tEPCYU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_latent_4(encoder, decoder):\n",
        "    n = 30\n",
        "    size = 28\n",
        "    scale = 2.0\n",
        "    figsize = 15\n",
        "    figure = np.zeros((size * n, size * n))\n",
        "    grid_x = np.linspace(-scale, scale, n)\n",
        "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[1, 0, xi, yi]])\n",
        "            x_decoded = decoder.predict(z_sample)\n",
        "            pic = x_decoded[0].reshape(size, size)\n",
        "            figure[\n",
        "                i * size : (i + 1) * size,\n",
        "                j * size : (j + 1) * size,\n",
        "            ] = pic\n",
        "\n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    start_range = size // 2\n",
        "    end_range = (n - 1) * size + start_range + 1\n",
        "    pixel_range = np.arange(start_range, end_range, size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[3]\")\n",
        "    plt.ylabel(\"z[4]\")\n",
        "    plt.imshow(figure, cmap=\"Greys_r\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_latent_4(encoder, decoder)"
      ],
      "metadata": {
        "id": "LywOS15ZCcIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_decoded = decoder.predict([[1, 0, -1.7, 1.7]])\n",
        "plt.imshow(x_decoded[0,:,:,0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KWoLFfi0Ce4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Сделайте краткий обзор научной работы, посвящённой алгоритму нейронных сетей, не рассматриваемому ранее на курсе. Проведите анализ: чем отличается выбранная архитектура от других? В чём плюсы и минусы данной архитектуры? Какие могут возникнуть трудности при её применении на практике?**"
      ],
      "metadata": {
        "id": "O4Lhl2KzCrjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Q Network (DQN)**"
      ],
      "metadata": {
        "id": "5cfDEJyZCxL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Об успехах Google Deepmind сейчас знают и говорят. Алгоритмы DQN (Deep Q-Network) побеждают Человека с неплохим отрывом всё в большее количество игр. Достижения последних лет впечатляют: буквально за десятки минут обучения, алгоритмы учатся выигрывать человека в понг и другие игры Atari. Недавно вышли в третье измерение — побеждают человека в DOOM в реальном времени, а также учатся управлять машинами и вертолетами.\n",
        "\n",
        "Алгоритм DQN (Deep Q-Network) algorithm впервые появился в 2015г. Он позволил добиться столь впечатляющих успехов путем сочетания Reinforcement learning (RL) с нейронными сетями глубокого обучения.\n",
        "\n",
        "DQN и Qleanring - аналогичные алгоритмы, основанные на итерации значения, но в обычном Q-обучении, когда состояние и пространство действия дискретны, а размерность невелика, Q-таблица может использоваться для хранения значения Q каждой пары действий состояния, и когда состояние и пространство действия непрерывны в высоком измерении, очень трудно использовать Q-таблицу без слишком большого пространства действия и состояния. Однако, можно перевести обновление Q-таблицы в проблему подбора функций, подгоняя функцию вместо Q-таблицы для генерации значений Q. В результате чего, похожие состояния будут вызывать похожие выходные действия.\n",
        "\n",
        "Таким образом, глубокая нейронная сеть хорошо влияет на извлечение сложных функций. Такое влияние даёт возможность комбинировать DeepLearning (DL) и Reinforcement Learning (RL)."
      ],
      "metadata": {
        "id": "EDzKno4hCyIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комбинация DL и RL имеет следующие проблемы:**"
      ],
      "metadata": {
        "id": "lU5r4pXYC1UP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Шум и задержка, в связи с этим во многих состояниях значения вознаграждения состояния равны 0, что является редкими выборками.\n",
        "- Каждый образец DL не зависит друг от друга, и значение текущего состояния RL зависит от возвращаемого значения последнего состояния.\n",
        "- При использовании нелинейной сети для представления функции значения, может возникнуть нестабильность\n",
        "\n",
        "Два основных инструмента в DQN решают вышеуказанные проблемы\n",
        "\n",
        "- Использование награды для создания тегов через Q-Learning\n",
        "- Воспроизведение опыта (пул опыта) для решения проблемы корреляции и нестатического распределения.\n",
        "\n",
        "Пул опыта в DQN используется для изучения предыдущего опыта, и поскольку Q обучение это метод автономного обучения то такой метод позволяет не только учиться на прошлом опыте но так же учиться без использования прошлого опыта, что уже почти фантастика."
      ],
      "metadata": {
        "id": "4oW-YCMNC3NA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Основные преимущества Deep Q Network**"
      ],
      "metadata": {
        "id": "4LMySZcuDCcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Случайное добавление предыдущего опыта в учебный процесс сделает нейронную сеть более эффективной.\n",
        "- Пул опыта решает проблему релевантности и нестатического распределения."
      ],
      "metadata": {
        "id": "Perq4B7ZDHiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Основной недостаток\n",
        "\n",
        "С ростом числа циклов обучения происходит резкое увеличение потребляемых ресурсов при этом ошибка уменьшается незначительно, а в некоторых случаях может и возрастать.\n",
        "\n",
        "В этом недостатке и заключается основная трудность в применении алгоритма на практике."
      ],
      "metadata": {
        "id": "G5XAwtRpDK-A"
      }
    }
  ]
}